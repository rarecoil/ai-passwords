# PassGAN

PassGAN is a relatively popular password research generation model, coming from the paper [_PassGAN: A Deep Learning Approach for Password Guessing_](https://arxiv.org/abs/1709.00440) \[[PDF](https://github.com/secml2018/secml2018.github.io/raw/master/PASSGAN_SECML2018.pdf)\]. It is one of the models mentioned by name in the hashcat 5.x release for slow candidates.

Given it is tailored specifically for password guessing, it is significantly faster than the misuse of GPT-2-small that exists in this repository.

## Experiment Setup

A two-year-old [implementation of PassGAN](https://github.com/brannondorsey/passgan) exists, which was the basis for this experiment. However, it requires Python 2.7, TensorFlow 1.4, and CUDA 8 to run. Installing these older variants in a [CUDA Docker container](https://hub.docker.com/r/nvidia/cuda/) is possible, but it is unsupported. Furthermore, the `requirements.txt` file was missing `pillow`, which is required for the `matplotlib` dependency. To solve all of these issues in one pass, I [forked the PassGAN repository](https://github.com/rarecoil/passgan) and updated it for Python 3.x, TensorFlow 1.15, and CUDA 10.0 as that is what is installed currently on the experimentation rig.

As the Radeon VII is in use training another model, PassGAN implementation was run on an Nvidia GeForce GTX 1070Ti 8GB card. Training data for this model is the same [hashes.org.10m.txt](https://github.com/rarecoil/ai-passwords/blob/master/gpt2-small/rawdata/hashes.org.10m.txt.gz) used for other models. This training data is the top 10 million plaintext passwords by occurrence in a snapshot of all of the hashes.org "founds" as of 1 November 2019, available [in this repository](https://github.com/rarecoil/hashes.org-list).

Training the model was relatively fast on this mid-level Pascal card, completing its 200,000 iterations sometime overnight. 

````bash
\$ python train.py --output-dir output --training-data hashes.org.10m.txt 
````

This training script outputs the model to `output/` for later use. Once training is completed, we can generate candidate passwords with the model also as easily, thanks to Brannon's code:

````bash
\$ python sample.py --input-dir output/ \ 
    --checkpoint output/checkpointscheckpoint_195000.ckpt \ 
    --output gen-passwords.txt \ 
    --batch-size 1024 \ 
    --num-samples 5000000
````

Generation is relatively fast on the 1070Ti as well. All 4,999,168 samples were generated in 59.15 seconds, for a rate of about 84,516 passwords per second.

## Results

After generation, the model had produced a total of 4,999,168 passwords. Using `sort -u` to receive a unique list, the model generated 4,658,237 unique passwords. 

We then calculate a series of intersections: first, with our training set, and second, with our full list of founds. The training set is known to us, so for cracking purposes this generation is wasted.

````bash
\$ grep -Fx -f gen-passwords.unique.txt hashes.org.10m.txt | wc -l
454759
````

With 454,759 passwords generated directly existing in our training set, we have a unique password occurrence of 90.23% across our unique generated set. Using our 84,516 passwords/sec above, we get about 76,259 unique passwords per second from this model.

### Performance against popular password lists

Instead of running hashcat and using GPGPU power, we can use the entire hashes.org "founds" list of 822,898,440 passwords and check our intersection with the greater list using the same `grep` construct as above:

````bash
\$ grep -Fx -f gen-passwords.unique.txt hashes.org.final.txt | wc -l
1169794
````

Note that this list still contains the duplicates, so the model generated `1169794 - 454759 =` 715,035 novel passwords that existed in the greater dataset.

## Conclusion

* 6.82% of all passwords generated over time had already been generated by the model (wasted computation)
* 9.09% of all passwords generated by the model are directly from its training set (wasted computation)
* 14.30% of all passwords generated by the model yield cracked passwords *not* in the training dataset (effectiveness)

Put another way, roughly 23.39% of passwords generated by the model yield a crack, or **19,726 cracked passwords per second.**

## Appendices

`rawdata/` contains the raw output from PassGAN without being deduplicated.

The `output/` directory generated from this report, including all intermediate checkpoints and samples, is a 900MB .xz file. It is available for download [from this S3 link](https://s3.rarecoil.com/data/ai-passwords/passgan.tar.xz).